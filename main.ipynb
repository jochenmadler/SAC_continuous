{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from SAC import SAC\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# create SAC agent\n",
    "config = {\n",
    "    'state_size': 10,\n",
    "    'action_size': (2,2), # tuple (t, k) where t*k actions are sampled from t k-dimensional distributions\n",
    "    'max_episode_steps': 20, # episode length\n",
    "    'max_steps': 20*200, # number of episodes\n",
    "    'min_steps': 20*1, # steps before training starts\n",
    "    'warmup': False, # random actions before training starts\n",
    "    'buffer_size': 20*160, # should be 80-100% of max_steps\n",
    "    'minibatch_size': 256, # should be 128-512\n",
    "    'update_interval': 1,\n",
    "    'validation_interval': 20*10, # validate performance every 10 episodes\n",
    "    'preprocess_state': False, # normalize state, implement on your own\n",
    "    'actor_weights_scaling': 0.1,\n",
    "    'activation_function': 'tanh',\n",
    "    'weights_initializer': 'glorot_uniform',\n",
    "    'pol_nr_layers': 2,\n",
    "    'pol_hidden_size': 16,\n",
    "    'val_nr_layers': 2,\n",
    "    'val_hidden_size': 16,\n",
    "    'gamma': 0.99,\n",
    "    'lr': 8e-4, # should be 1e-4-1e-3\n",
    "    'alpha_init': 0.001, # requires careful tuning: monitor output std.\n",
    "    'alpha_lr': 0.0, # hard to converge, should be <= lr\n",
    "    'alpha_decay_rate': 0.0, # depends on max_steps\n",
    "    'alpha_to_zero_steps': 20*100, # should be ca. 40-60% of max_steps\n",
    "    'polyak': 0.995,\n",
    "    'huber_delta': 2.0,\n",
    "    'gradient_clip_norm': 2.0, # should be 2.0-5.0\n",
    "    'reg_coef': 0.0,\n",
    "    'std_initial_value': 0.4, # should be 0.25-0.5\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "\n",
    "# define test environment\n",
    "class Env():\n",
    "    def __init__(self, agent, target=0.6):\n",
    "        self.agent = agent\n",
    "        self.state_size = self.agent.state_size\n",
    "        self.target = target\n",
    "        \n",
    "    def get_state(self):\n",
    "        return tf.ones((1,self.state_size), dtype=tf.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.get_state()\n",
    "\n",
    "    # reward is MAE between action and target, scaled to be between 0.0 and 1.0\n",
    "    def step(self, action, target=0.6):\n",
    "        target_ = tf.ones_like(action) * target\n",
    "        mae = tf.reduce_mean(tf.abs(action - target_))\n",
    "        reward = tf.clip_by_value(1. - mae, 0.0, 1.0)\n",
    "        next_state = self.get_state()\n",
    "        return next_state, reward\n",
    "\n",
    "\n",
    "# create environment and agent\n",
    "agent = SAC(config)\n",
    "env = Env(agent)\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:      200 (  5.0%) - validation reward: 0.70\n",
      "step:      400 ( 10.0%) - validation reward: 0.66\n",
      "step:      600 ( 15.0%) - validation reward: 0.65\n",
      "step:      800 ( 20.0%) - validation reward: 0.60\n",
      "step:     1000 ( 25.0%) - validation reward: 0.67\n",
      "step:     1200 ( 30.0%) - validation reward: 0.68\n",
      "step:     1400 ( 35.0%) - validation reward: 0.74\n",
      "step:     1600 ( 40.0%) - validation reward: 0.74\n",
      "step:     1800 ( 45.0%) - validation reward: 0.98\n",
      "step:     2000 ( 50.0%) - validation reward: 0.86\n",
      "step:     2200 ( 55.0%) - validation reward: 0.91\n",
      "step:     2400 ( 60.0%) - validation reward: 0.90\n",
      "step:     2600 ( 65.0%) - validation reward: 0.90\n",
      "step:     2800 ( 70.0%) - validation reward: 0.90\n",
      "step:     3000 ( 75.0%) - validation reward: 0.90\n",
      "step:     3200 ( 80.0%) - validation reward: 0.90\n",
      "step:     3400 ( 85.0%) - validation reward: 0.90\n",
      "step:     3600 ( 90.0%) - validation reward: 0.92\n",
      "step:     3800 ( 95.0%) - validation reward: 0.94\n",
      "step:     4000 (100.0%) - validation reward: 0.97\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "total_steps = 0\n",
    "episode_steps = 0\n",
    "observation = env.reset()\n",
    "\n",
    "while total_steps < agent.max_steps:\n",
    "    \n",
    "    # schedule alpha temperature parameter: decay and/or set to zero\n",
    "    agent.manage_alpha_value(total_steps)\n",
    "            \n",
    "    # action selection, depending on warmup\n",
    "    if total_steps < agent.warmup_steps:\n",
    "        mean, std, action, raw_action = agent.random_action()\n",
    "    else:\n",
    "        mean, std, action, _, _, raw_action = agent.select_action(observation, training=True)\n",
    "        \n",
    "    next_observation, reward = env.step(action)\n",
    "    agent.buffer.store(observation, action, reward, next_observation)\n",
    "    observation = next_observation\n",
    "    total_steps += 1\n",
    "    episode_steps += 1\n",
    "    \n",
    "    # at the end of the episode, reset\n",
    "    if episode_steps == agent.max_episode_steps:\n",
    "        observation = env.reset()\n",
    "        episode_steps = 0\n",
    "        \n",
    "    # update agent (policy and value networks)\n",
    "    if total_steps > agent.min_steps and total_steps % agent.update_interval == 0:\n",
    "        q1_loss, q2_loss, min_q, policy_loss, min_logpi, max_logpi, mean_logpi, alpha_loss, entropy, grads_norm = agent.train()\n",
    "        \n",
    "    # validate agent (optional)\n",
    "    if agent.validation_interval > 0 and total_steps % agent.validation_interval == 0:\n",
    "        observation, val_rews = env.reset(), []\n",
    "        for _ in range(agent.max_episode_steps):\n",
    "            mean, std, action, _, _, _ = agent.select_action(observation, training=False)\n",
    "            next_observation, reward = env.step(action)\n",
    "            observation = next_observation\n",
    "            val_rews.append(reward)\n",
    "        \n",
    "        observation = env.reset()\n",
    "        print(f'step: {total_steps:>8} ({(total_steps/agent.max_steps):>6.1%}) - validation reward: {np.mean(val_rews):.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
